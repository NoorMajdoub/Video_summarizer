{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12205315,"sourceType":"datasetVersion","datasetId":7688556},{"sourceId":12205701,"sourceType":"datasetVersion","datasetId":7688831},{"sourceId":12205751,"sourceType":"datasetVersion","datasetId":7688868},{"sourceId":12205824,"sourceType":"datasetVersion","datasetId":7688914}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#goal : getting stuff from video , \n#connection them both\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# understand how do with frame processing , do you implment by yourself frame by frame ? is there sht that detect\n#chages","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## goal goal of this notebook == collect code from video when video , like continious \n#conntected clear full cide\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## testing out vit","metadata":{}},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, ViViTForVideoClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\nmodel = ViViTForVideoClassification.from_pretrained(\"google/vivit-b-16x2-kinetics400\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T10:40:30.917174Z","iopub.execute_input":"2025-06-18T10:40:30.917412Z","iopub.status.idle":"2025-06-18T10:40:42.717663Z","shell.execute_reply.started":"2025-06-18T10:40:30.917386Z","shell.execute_reply":"2025-06-18T10:40:42.716590Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/474932859.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load model directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mViViTForVideoClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google/vivit-b-16x2-kinetics400\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViViTForVideoClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google/vivit-b-16x2-kinetics400\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'ViViTForVideoClassification' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'ViViTForVideoClassification' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"## testing fc to take ocr\n## testing function to work on vid and get frames and somehow combines all code from vid if there is ","metadata":{}},{"cell_type":"markdown","source":"\n## Qwen2-VL-OCR-2B-Instruct \n\n\n It focuses on reading and understanding text within the image., provide configuration to know how to use","metadata":{}},{"cell_type":"code","source":"!pip install pytesseract","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T10:40:50.746833Z","iopub.execute_input":"2025-06-18T10:40:50.747428Z","iopub.status.idle":"2025-06-18T10:40:54.505161Z","shell.execute_reply.started":"2025-06-18T10:40:50.747402Z","shell.execute_reply":"2025-06-18T10:40:54.504268Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\nRequirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import cv2\nimport pytesseract\nfrom PIL import Image\n\nimg_cv = cv2.imread(r'/kaggle/input/videodata/Screenshot 2025-06-18 111014.png')\n\n# By default OpenCV stores images in BGR format and since pytesseract assumes RGB format,\n# we need to convert from BGR to RGB format/mode:\nimg_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\nprint(pytesseract.image_to_string(img_rgb))\n# OR\nimg_rgb = Image.frombytes('RGB', img_cv.shape[:2], img_cv, 'raw', 'BGR', 0, 0)\nprint(pytesseract.image_to_string(img_rgb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T10:19:17.687258Z","iopub.execute_input":"2025-06-18T10:19:17.687713Z","iopub.status.idle":"2025-06-18T10:19:21.015499Z","shell.execute_reply.started":"2025-06-18T10:19:17.687685Z","shell.execute_reply":"2025-06-18T10:19:21.014128Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":" \n\n \n\neerreg\n\n \n\n \n\n \n\nCee\n\nCee Sea ee ae\nere Le Coe oD\ncetera eC)\n\n \n\n \n\n \n\n‘and auctions properties and hands out the~r proper Title Deed cards;\n\n \n\nPO Ra Ors\n\ni\n\nAta ee aor cee id\nGames, Consumer Affairs Dept., P.0. Box 200, Pawtucket, RI @2862. Tel: 888-896-7025\n(Core Cee ee Rae ee tae ee Ty\nerm ete et eC Ce\n\n‘The HASBRO, PARKER BROTHERS, and MONOPOLY nanes and logos, the distinctive design of\nCr A ee a a he er cae er\nCO Core mC ka a Rca on namic’\nSree A ene aa Lee\n\nOa tea ee Ra cs amnion\n\nCel\n\nCe aad\n\n \n\n \n\nCSC Rune a eee oO a ae CC eau anc Le aca)\n\nFee OL as ee tn Un oe\nsere eyo a OCA\n\nSe ee)\neve eNSU a en RRO ote\n\nCoe OC Rec Rc Ree ad\n\n      \n\nene Ee LO aE\n\n \n\nDee ELEC\n\nCd aaah eee Ome Ec Oe Om su ene eu\neo\n\nSn rect Rp reo vera Rrra\n\nCEU URC ae sce se CCR Ua COU eeu eC oUeC Me\ntimes, but a given plastic train may never be used twice in the same continuous path. In the case of a tie for the longestpath, all tied players sc\nThe player with the most points wins the game. If two or more players are tied for the most points, the player who has completed the most Destinati\nSect On cme es a eset ioe ce wen aust cue Rote\n\npaar pica alee ey\n\n \n\f\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\f\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pytesseract\nfrom PIL import Image\n\ntext = pytesseract.image_to_string(Image.open('/kaggle/input/videodata/Screenshot 2025-06-18 111014.png'), config='--psm 6')\nprint(text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T10:54:31.262486Z","iopub.execute_input":"2025-06-18T10:54:31.262770Z","iopub.status.idle":"2025-06-18T10:54:33.486250Z","shell.execute_reply.started":"2025-06-18T10:54:31.262750Z","shell.execute_reply":"2025-06-18T10:54:33.485490Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"peo ered og\na) ee ee eee ed $\nra\nitr ee Ru ee a me\n[eee Actual Response: {actual_response}\ni.\nee ec a a er Cn nod\nees _\nropa ‘end money to another player.\ner oe ee eee ee iy\neareay Pee ot eM eM eta Oe\neee Cote roe ctr CR koro Caer ery\nae ere ent ik tere eae\nacon The HASBRO, PARKER BROTHERS, and MONOPOLY nanes and logos, the distinctive design of\nretry eee ee ee ec caer ea\ndistinctive elenents of the board and rules are tradenarks of Hasbro for its property trading gane\nSorta an a at\nie erence iene or coe\nal\nea ca\nRe oe nO Eee ae ne Sac)\nSee Onan ern RUN oe\nSources: ['data/monopoly.pdf:1:1\",'‘data/monopoly.pdf:1:0\", ‘data/monopoly.pdf:0:0', ‘data/monopoly.pdf:2:0\", ‘data/monopoly.pdf:7:2\"]\nCc)\nActual Response: A player starts with $1,500 in Honopoly.\nCera Oca ec eee ec ee ctr\nCd aaah eee Ome Ec Oe Om su ene eu\nns\nPosen Onset ae Oecd\nCE nC aL One aCe L ae Ue Rou Lust Be Ma\ntines, but a given plastic train may never be used twice in the sane continuous path. In the case of a tie for the longestpath, all tied players sc\nThe player with the most points wins the gane. If two or nore players are tied for the most points, the player who has completed the most Destinati\nTickets wins. In the unlikely event that they are still tied, the player with the Longest Continuous Path card wins.\nbitchy Ae ela adnate\n\f\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#CodeBERT detect if code or not , but cant get texxxxxxxxxxxxxxxxxxxxxxxxxxxxxt whyyyyyyy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T10:46:12.191792Z","iopub.execute_input":"2025-06-18T10:46:12.192494Z","iopub.status.idle":"2025-06-18T10:46:12.197213Z","shell.execute_reply.started":"2025-06-18T10:46:12.192462Z","shell.execute_reply":"2025-06-18T10:46:12.196445Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"/kaggle/input/videodata/Screenshot 2025-06-18 111014.png\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T10:46:37.556996Z","iopub.execute_input":"2025-06-18T10:46:37.557278Z","iopub.status.idle":"2025-06-18T10:47:13.822684Z","shell.execute_reply.started":"2025-06-18T10:46:37.557255Z","shell.execute_reply":"2025-06-18T10:47:13.821853Z"}},"outputs":[{"name":"stdout","text":"['The image shows a screenshot of a computer terminal with a code editor open. The code editor is displaying two files: \"query_data.py\" and \"test_rag.py\". The terminal window is divided into two sections: the left section shows the code editor, and the right section displays the terminal output. The left section contains the code for two files, with the first file being \"query_data.py\" and the second file being \"test_rag.py\". The right section shows the terminal output, which includes the expected and actual responses to a question about the game \"Monopoly\".<|im_end|>']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install qwen_vl_utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T10:41:21.473302Z","iopub.execute_input":"2025-06-18T10:41:21.474035Z","iopub.status.idle":"2025-06-18T10:41:26.092847Z","shell.execute_reply.started":"2025-06-18T10:41:21.473995Z","shell.execute_reply":"2025-06-18T10:41:26.091985Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting qwen_vl_utils\n  Downloading qwen_vl_utils-0.0.11-py3-none-any.whl.metadata (6.3 kB)\nCollecting av (from qwen_vl_utils)\n  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from qwen_vl_utils) (25.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen_vl_utils) (11.1.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from qwen_vl_utils) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (2025.4.26)\nDownloading qwen_vl_utils-0.0.11-py3-none-any.whl (7.6 kB)\nDownloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: av, qwen_vl_utils\nSuccessfully installed av-14.4.0 qwen_vl_utils-0.0.11\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"/kaggle/input/screenshotdata3/Screenshot 2025-06-18 115631.png\",\n            },\n            {\"type\": \"text\", \"text\": \"Extract All the code from this image\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n     padding=True,\n    return_tensors=\"pt\",\n)\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:13:53.742508Z","iopub.execute_input":"2025-06-18T11:13:53.742802Z","iopub.status.idle":"2025-06-18T11:14:06.649695Z","shell.execute_reply.started":"2025-06-18T11:13:53.742777Z","shell.execute_reply":"2025-06-18T11:14:06.648934Z"}},"outputs":[{"name":"stdout","text":"['@validator(\"account_id\")\\ndef validate_account_id(cls, value):\\n    if value <= 0:\\n        raise ValueError(f\"account_id must be positive: {value}\")\\n    return value<|im_end|>']\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"text = pytesseract.image_to_string(Image.open('/kaggle/input/screenshotdata2/Screenshot 2025-06-18 114936.png'), config='--psm 6')\nprint(text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T10:54:39.039434Z","iopub.execute_input":"2025-06-18T10:54:39.039701Z","iopub.status.idle":"2025-06-18T10:54:39.874788Z","shell.execute_reply.started":"2025-06-18T10:54:39.039680Z","shell.execute_reply":"2025-06-18T10:54:39.874072Z"}},"outputs":[{"name":"stdout","text":"tl = time.time()\n\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n‘inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n\n‘inputs = {k: v.to(DEVICE) for k, v in inputs. items()}\n\f\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"text = pytesseract.image_to_string(Image.open('/kaggle/input/screenshotdata3/Screenshot 2025-06-18 115631.png'), config='--psm 6')\nprint(text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T10:57:23.652383Z","iopub.execute_input":"2025-06-18T10:57:23.652693Z","iopub.status.idle":"2025-06-18T10:57:24.456709Z","shell.execute_reply.started":"2025-06-18T10:57:23.652670Z","shell.execute_reply":"2025-06-18T10:57:24.456009Z"}},"outputs":[{"name":"stdout","text":"5 di et I EE ===\n= ee8@ ge\n= @validator(\"account_id\") ge\n= def validate_account_id(cls, value): :\nif value <= 0: 5\nraise ValueError(f\"account_id must be positive: {value}\") :\nia-na0 NO) g\n= =F} y\n= ee ee 2)\n\f\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"#how many frames we talking ?\nimport cv2\n\n# Load the video\nvideo_path = \"/kaggle/input/videodata2/How to land a Google internship.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Get the total number of frames\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\ncap.release()\n\nprint(f\"Total frames: {frame_count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:05:02.357626Z","iopub.execute_input":"2025-06-18T11:05:02.357922Z","iopub.status.idle":"2025-06-18T11:05:02.437219Z","shell.execute_reply.started":"2025-06-18T11:05:02.357903Z","shell.execute_reply":"2025-06-18T11:05:02.436524Z"}},"outputs":[{"name":"stdout","text":"Total frames: 4647\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"fps = cap.get(cv2.CAP_PROP_FPS)\nduration_sec = frame_count / fps\nprint(f\"FPS: {fps}\")\nprint(f\"Duration (s): {duration_sec}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:05:25.085098Z","iopub.execute_input":"2025-06-18T11:05:25.085379Z","iopub.status.idle":"2025-06-18T11:05:25.107128Z","shell.execute_reply.started":"2025-06-18T11:05:25.085358Z","shell.execute_reply":"2025-06-18T11:05:25.106208Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2345950055.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mduration_sec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_count\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"FPS: {fps}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Duration (s): {duration_sec}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"],"ename":"ZeroDivisionError","evalue":"float division by zero","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"# 4647 /100  how to calculate how many times to inspect frames based on how many framew we have\n\n# https://huggingface.co/spaces/prithivMLmods/Multimodal-OCR\n#https://huggingface.co/Qwen/Qwen2-VL-2B\n\n\n\n#for when wanna get code ,make the time frames selon what you understand from the transcription \n#use context you get from the transcription and and synchronize it with frames ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}